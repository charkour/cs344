{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# A Dive into Reinforcement Learning: Artificial Intelligence Project Proposal\n",
    "\n",
    "By Charles Kornoelje\n",
    "\n",
    "Updated 05/15/2020\n",
    "\n",
    "## Vision\n",
    "\n",
    "The goal of my CS 344 honors final project is to take a deep dive into [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning), with the hope of training an artificial intelligence agent to play a video game. The first agent I was able to implement was with a [deep Q-learning network](https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning) (DQN) designed to play the Atari 2600 game, [_Breakout_]( https://en.wikipedia.org/wiki/Breakout_(video_game) ), the classic brick-breaking game. However, I quickly learned that training a somewhat intelligent agent would take lots of computational time and energy, something of which I do not have. So I began training an agent and moved on to find a game that took less power, which led me to the text-based video game, _[FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/)_. I was able to train a smart agent to play the game after following a guide and finding some code.\n",
    "\n",
    "The purpose of the project is to learn how to use reinforcement learning to train agents. Reinforcement learning is a domain of machine learning where an agent takes actions based on observations in their environment to maximize their reward. The project falls under the active reinforcement learning realm in which a Q-learning agent is trained with an action-utility function (Q-function) to learn a control policy that tells an agent which actions to take at a current state. Learning the control policy will assist the agent in decision making in order to take proper actions to maximize their score in video games. If an agent is able to be trained to play a game well, the same training can be applied to real life activities and techniques.\n",
    "\n",
    "## Background\n",
    "\n",
    "Both the _Breakout_ and _FrozenLake_ games are environments from the [OpenAI](https://openai.com/) [Gym Python package](https://gym.openai.com/). The Gym versions of the games make it easy to interface with modern machine learning frameworks, such as [Keras](https://keras.io/) and [TensorFlow](https://www.tensorflow.org/), which I will be using to train my artificial intelligence agent. I chose these technologies because I have previous experience with them and the guides I follow implement the reinforcement algorithms with them. The artificial agent will be trained using a deep neural network using reinforcement learning algorithms.\n",
    "\n",
    "### Reinforcement Learning\n",
    "\n",
    "My basic understanding of reinforcement learning came from Chapter 21 of _[Artificial Intelligence: A Modern Approach, Third Edition](http://aima.cs.berkeley.edu/)_ by Russell and Norvig. Reinforcement learning provides feedback to an agent as to whether its action is good or bad, and from there, it will update its behavior to maximize its reward. The agent begins without knowing which actions lead to desirable outcomes and which do not, and over time, the agent will begin to adjust its behavior to maximize its reward. Reinforcement learning assumes a fully-observable environment, which makes it especially applicable to training video game bots where the game state and all possible actions are known. It is also assumed that the agent does not know anything about the environment or what actions it should take, only what actions it may take. It decides what actions to take based on the Markov decision process (TODO: elaborate on this?).\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "The agent can be designed in many different ways for reinforcement learning, but I will focus on a Q-learning agent. This agent learns an action-utility function, or Q-function, that gives the expected value for taking an action given the current state (CITE: Textbook 831). Russell and Norvig state, “A Q-learning agent, on the other hand, can compare the expected utilities for its available choices without needing to know their outcomes, so it does not need a model of the environment. On the other hand, because they do not know where their actions lead, Q-learning agents cannot look ahead; this can seriously restrict their ability to learn” (CITE: textbook 831). Like any agent implementation, there are pros and cons, but allowing agents to compare expected values from choices without knowing their outcome is good for video game play that has non-deterministic outcome. We can use a Q-function to update itself over an iterative process to calculate exact Q-values when given an estimated model.\n",
    "\n",
    "### Deep Q-Learning Network (DQN)\n",
    "\n",
    "According to [Mnih et al. (2013)](https://arxiv.org/pdf/1312.5602.pdf), deep reinforcement learning can be achieved through a combination of a deep neural network and a Q-learning function, resulting in a deep Q-learning network (DQN). The DQN exists of some sort of memory that is a set of tuples containing (state, action, reward, next state) and an action-value Q-function initialized to random weights. In the basic sense, a DQN uses a Q-function to update the weights in the deep neural network that correspond to s state and action.\n",
    "\n",
    "### Double DQN (DDQN)\n",
    "\n",
    "According to [van Hasselt et al. (2015)](https://arxiv.org/pdf/1509.06461.pdf), “The popular Q-learning algorithm is known to overestimate action values under certain conditions” so two value functions are learned instead of one. The first value is estimating the value of the policy wanting to maximize the reward, and then the second value to fairly evaluate the value of the first policy. This leads to less overestimations and provides “more stable and reliable learning” (van Hasselt et al.).\n",
    "\n",
    "## Implementation\n",
    "\n",
    "\n",
    "### _Breakout_ Reinforcement Learning\n",
    "\n",
    "To train the agent to play _Breakout_, I followed the article “_[Beat Atari with Deep Reinforcement Learning! (Part 1: DQN)](https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26)_” by Adrien Lucas Ecoffet, which provided a solid overview of the idea of Q-learning, but failed to provide a detailed enough explanation of code implementation. One of the comments responded with their [GitHub repo implementation](https://github.com/boyuanf/DeepQLearning) of a DQN to play _Breakout_. After slightly modifying boyuanf’s code, I was able to get it running on my machine. The DQN is trained by an array representation of the current screen state where each pixel has an RGB value, with the shape of the array being (210, 160, 3). For each state, there is an integer value that reinforces each action, with positive integers being positive reinforcement. I quickly realized that Q-learning involves a lot of math and custom functions that are specific to the problem, but I did not know how to build it on my own yet. I have added my [updated version of the code](https://github.com/charkour/cs344/blob/master/project/research-and-examples/boyuan-dqn-example.py) to my project directory. The major change I made was to lower the amount of previous actions and responses remembered ten fold. Previously, boyuanf was storing 20 GBs of past decisions and rewards, but I felt that was too much and having less than that would help the agent find better decisions more quickly. The artitecture starts with a normalized layer of the (210, 160, 3) input, then two convolutional layers, which are flattened into a dense layer with 256 rectifier units, and then another dense layer the size of the actions, which is 3, and then goes into a filtered output that applies a mask to get one action. There are over 600,000 parameters that are estimated in the model.\n",
    "\n",
    "*   [gym](https://gym.openai.com/)\n",
    "*   [numpy](https://numpy.org/)\n",
    "*   [tensorflow](https://www.tensorflow.org/)\n",
    "*   [keras](https://keras.io/)\n",
    "*   [skimage](https://scikit-image.org/) (for preprocessing)\n",
    "*   [Collections](https://docs.python.org/3.6/library/collections.html#collections.deque) (for deque)\n",
    "\n",
    "I quickly realized that my personal machine (a 2015 MacBook Pro with a 3.1 GHz Dual-Core Intel i7 processor and 16 GB DDR3 RAM) would not have enough CPU power to train the DQN to play breakout in a reasonable amount of time. I tried Google Colab, but that was not much better. I was able to connect to one of Calvin’s lab machines and start training the model (TODO: give the specs?). After starting the training of the model, I moved onto trying to find a better article related to deep Q-learning for game playing.\n",
    "\n",
    "### _FrozenLake_ Reinforcement Learning\n",
    "\n",
    "The _FrozenLake_ board is 4x4, where there is a start (S) space in the top left, and a goal (G) space in the bottom right. The rest of the spaces are a random assortment of frozen (F) spaces, that are safe to step on, and hole (H) spaces, that are not safe for the player to step on, and will cause them to lose the game. Successfully traversing from S to G on F spaces will reward the agent positively. For every step taken the reward is 0, for falling in a hole the reward is 0, and 1 for reaching the goal. There are four actions the agent can take, each action is moving in one of the cardinal directions. At each current state, the DQN estimates the reward for each action and takes the best one. Overtime, the agent will learn that reaching the goal state is ideal because it receives a reward. One episode is one attempt at the game, which will either be a success (with a reward of 1) or a failure (0).\n",
    "\n",
    "Determined not to give up on my conquest of training an video-game-playing agent with deep Q-learning, my research led me to an article “[Bias-Variance for Deep Reinforcement Learning: How To Build a Bot for Atari with OpenAI Gym](https://www.digitalocean.com/community/tutorials/how-to-build-atari-bot-with-openai-gym#step-6-%E2%80%94-creating-a-deep-q-learning-agent-for-space-invaders)” by Alvin Wan. From his tutorials, I was able to train an agent to play the game well, but I could not understand where the DNN was implemented in his code. To me it seemed like it was training just through using a gradient descent optimizer and a graph search to minimise error, which is not technically making it a deep network, but is still Q-learning I believe.\n",
    "\n",
    "After Wan’s article, I searched for a Keras implementation of a DQN for _FrozenLake_, and found a [StackOverflow post](https://stackoverflow.com/questions/45869939/something-wrong-with-keras-code-q-learning-openai-gym-frozenlake) which lead to a [Jupyter Notebook](https://gist.github.com/weiji14/bab587907681869ec0f70f7496f98a12), which referenced a [Keras DQN implementation for OpenAI’s FrozenLake](https://gist.github.com/ceshine/eeb97564c21a77b8c315179f82b3fc08), by GitHub user CeShine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: CeShine\n",
    "@author: Charkour\n",
    "Updated to work for 8x8 frozen ice.\n",
    "Ability to load weights.\n",
    "Fix small bugs for new versions.\n",
    "Reuseability features.\n",
    "\n",
    "Using keras-rl (https://github.com/matthiasplappert/keras-rl) to provide basic framework,\n",
    "and embedding layer to make it essentially a Q-table lookup algorithm.\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import Policy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "class DecayEpsGreedyQPolicy(Policy):\n",
    "\n",
    "    def __init__(self, max_eps=.1, min_eps=.05, lamb=0.001):\n",
    "        super(DecayEpsGreedyQPolicy, self).__init__()\n",
    "        self.max_eps = max_eps\n",
    "        self.lambd = lamb\n",
    "        self._steps = 0\n",
    "        self.min_eps = min_eps\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        assert q_values.ndim == 1\n",
    "        nb_actions = q_values.shape[0]\n",
    "        eps = self.min_eps + (self.max_eps - self.min_eps) * \\\n",
    "            np.exp(-self.lambd * self._steps)\n",
    "        self._steps += 1\n",
    "        if self._steps % 1e3 == 0:\n",
    "            print(\"Current eps:\", eps)\n",
    "        if np.random.uniform() < eps:\n",
    "            action = np.random.random_integers(0, nb_actions - 1)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        return action\n",
    "\n",
    "\n",
    "ENV_NAME = 'FrozenLake8x8-v0'\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "def get_keras_model(action_space_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(64, 4, input_length=1))\n",
    "    model.add(Reshape((4,)))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = get_keras_model(nb_actions)\n",
    "\n",
    "memory = SequentialMemory(window_length=1, limit=10000)\n",
    "policy = DecayEpsGreedyQPolicy(max_eps=0.9, min_eps=0, lamb=1 / (1e4))\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions,\n",
    "               memory=memory, nb_steps_warmup=500,\n",
    "               target_model_update=1e-2, policy=policy,\n",
    "               enable_double_dqn=False, batch_size=512\n",
    "               )\n",
    "dqn.compile(Adam())\n",
    "\n",
    "\n",
    "try:\n",
    "    dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "temp_folder = tempfile.mkdtemp()\n",
    "env = env.unwrapped\n",
    "# env = gym.wrappers.Monitor(env, directory=temp_folder, force=True, write_upon_reset=True)\n",
    "# env.monitor.start(temp_folder)\n",
    "\n",
    "# dqn.fit(env, nb_steps=1e5, visualize=False, verbose=1, log_interval=10000)\n",
    "# # dqn.fit(env, nb_steps=100, visualize=False, verbose=1, log_interval=100)\n",
    "#\n",
    "# # After training is done, we save the final weights.\n",
    "# dqn.save_weights('dqn_{}_weights1.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "dqn.load_weights(\"./dqn_FrozenLake8x8-v0_weights1.h5f\")\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=100, visualize=False)\n",
    "# env.monitor.close()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In its current state, this has extended upon CeShine’s work to play the 8x8 grid version of _FrozenLake_. My future work will be to update and optimize the DNN to solve the 8x8 version with higher accuracy than it currently does. Some work will need to be done to first calculate the accuracy of a trained model and then train the model multiple times to compare the performance.\n",
    "\n",
    "## Results\n",
    "\n",
    "\n",
    "### _FrozenLake_\n",
    "\n",
    "Currently, the Q-learning agent is able to complete the _FrozenLake_ game 75 times out of 100 attempts when training for 100,000 episodes. In the reinforcement learning domain, “solving” the puzzle is anything above 78 times, but I am not sure why this is considered the standard for assessing the ability of an artificial intelligence agent. Compared to Wan’s implementation of q-learning that achieves 82/100, CeShine’s example gets 80/100. However, Wan’s calculation takes the 100 best episodes while training for 4000 episodes, and CeShine’s trains for 100,000 episodes, but it takes the last 100 episodes. My extension of CeShine’s DQN for the 8x8 version solves the puzzle 75/100 times. I would like to see if the 8x8 performance can be increased by tweaking the network architecture or hyperparameters.\n",
    "\n",
    "\n",
    "### _Breakout_\n",
    "\n",
    "Additionally, I am excited to report the progress of the agent training on the campus lab computer. It has been training for 53 hours, completing over 6.35 million iterations through the neural network to estimate the agent’s future reward if an action is taken. I am surprised with the progress it has made. The highest score I have seen recently is 38 achieved on episode 16335, and it can usually score 5 or more while on episode 16400. This is exciting progress and it is performing better than boyuanf’s agents trained for 24 hours and 36 hours which got 0 and 11 respectively. I think this is due to the fact that I lowered the memory tenfold from what he had. I do not think I will continue working on this, but focus my work on the _FozenLake_ text game, which takes relatively less computational time and energy.\n",
    "\n",
    "Update 1: Now the best is 48 on episode 19784. 7.7 million steps.\n",
    "\n",
    "Update 2: Now the best is 58 on episode 37646. 18327920 steps.\n",
    "\n",
    "I started on Tuesday, May 5 at 3:50pm and stopped training on Tuesday, May 12 at 9:12am for a total of 161 training hours. 19,502,978 million steps were completed. The final model was able to score 11 points, which is the same as boyuanf’s model. Because of the way the models are saved, if I look at younger models, the score might be higher.\n",
    "\n",
    "\n",
    "## Implications\n",
    "\n",
    "Reinforcement learning is an interesting approach to training intelligent agents. This project has shown that with enough time and power, an artificial agent can be trained to play a simple task by just giving rewards. Although this approach may not give as good of results in the same amount of time as say supervised learning, reinforcement learning allows that agnet to be trained without massive amounts of meticulously curated and labeled data. One problem with reinforcement learning data, is having to define an action set and give rewards for certain actions or sequences of actions. If we are able to train computers to complete tasks in video games, then it is apparent that we can do this in real life too. We already do reinforcement learning in our normal lives, such as training a pet, and we will continue to do it with machines.\n",
    "\n",
    "I have learned that it takes a lot of time and energy to perform Q-learning, but as technology advances, it seems like it will be more easily achieved. Some really great work has come from reinforcement learning techniques, and I think they will just be getting better. I feel that reinforcement learning relates very closely with how humans act and learn as we get rewards or consequences for our actions. And before every action, we deliberate (sometimes not enough) on what we think our reward will be. However, in the current code setup, I do not believe the Q-learning agents have a concept of realizing long-term rewards like humans are able to. Yet, I think it is possible to train machines with long-term effects, but coding it and training it will take even more time.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6334182b",
   "language": "python",
   "display_name": "PyCharm (344)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
